#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ETL Pipeline - Construction du Data Warehouse"""

import pandas as pd
import numpy as np
from datetime import datetime
import sqlite3
import os
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataWarehouseETL:
    def __init__(self, source_path='01_data/raw/', target_path='01_data/warehouse/'):
        self.source_path = source_path
        self.target_path = target_path
        self.dataframes = {}
        os.makedirs(target_path, exist_ok=True)
        
    def extract(self):
        logger.info("="*60)
        logger.info("PHASE 1: EXTRACTION DES DONN√âES")
        logger.info("="*60)
        
        files = {
            'ventes': 'ventes.csv',
            'produits': 'produits.csv',
            'categories': 'categories.csv',
            'enseignes': 'enseignes.csv'
        }
        
        for name, file in files.items():
            df = pd.read_csv(f"{self.source_path}{file}")
            self.dataframes[name] = df
            logger.info(f"‚úÖ {name}: {len(df)} lignes extraites")
    
    def transform(self):
        logger.info("\nPHASE 2: TRANSFORMATION DES DONN√âES")
        logger.info("="*60)
        
        # DIM_TEMPS
        logger.info("üìÖ Cr√©ation de DIM_TEMPS...")
        ventes = self.dataframes['ventes']
        ventes['Date_vente'] = pd.to_datetime(ventes['Date_vente'])
        
        dates = pd.date_range(
            start=ventes['Date_vente'].min(),
            end=ventes['Date_vente'].max(),
            freq='D'
        )
        
        dim_temps = pd.DataFrame({
            'id_temps': range(1, len(dates) + 1),
            'date_complete': dates,
            'annee': dates.year,
            'mois': dates.month,
            'jour': dates.day,
            'trimestre': dates.quarter,
            'semaine': dates.isocalendar().week,
            'jour_semaine': dates.dayofweek + 1,
            'nom_jour': dates.strftime('%A'),
            'nom_mois': dates.strftime('%B'),
            'est_weekend': (dates.dayofweek >= 5).astype(int)
        })
        logger.info(f"  ‚úÖ {len(dim_temps)} dates cr√©√©es")
        
        # DIM_PRODUIT
        logger.info("üì¶ Cr√©ation de DIM_PRODUIT...")
        produits = self.dataframes['produits']
        categories = self.dataframes['categories']
        
        dim_produit = produits.merge(categories, on='Reference_Categorie', how='left')
        dim_produit['id_produit'] = range(1, len(dim_produit) + 1)
        
        dim_produit['gamme_prix'] = pd.cut(
            dim_produit['Prix'],
            bins=[0, 100, 500, 1000, float('inf')],
            labels=['√âconomique', 'Standard', 'Premium', 'Luxe']
        )
        logger.info(f"  ‚úÖ {len(dim_produit)} produits transform√©s")
        
        # DIM_ENSEIGNE
        logger.info("üè™ Cr√©ation de DIM_ENSEIGNE...")
        enseignes = self.dataframes['enseignes']
        dim_enseigne = enseignes.copy()
        dim_enseigne['id_enseigne'] = range(1, len(dim_enseigne) + 1)
        dim_enseigne['region'] = dim_enseigne['Ville'].map({
            'Paris': '√éle-de-France',
            'Lyon': 'Auvergne-Rh√¥ne-Alpes',
            'Marseille': 'Provence-Alpes-C√¥te d\'Azur',
            'Toulouse': 'Occitanie',
            'Nice': 'Provence-Alpes-C√¥te d\'Azur'
        })
        logger.info(f"  ‚úÖ {len(dim_enseigne)} enseignes transform√©es")
        
        # DIM_CATEGORIE
        logger.info("üè∑Ô∏è Cr√©ation de DIM_CATEGORIE...")
        dim_categorie = categories.copy()
        dim_categorie['id_categorie'] = range(1, len(dim_categorie) + 1)
        dim_categorie['super_categorie'] = dim_categorie['Categorie'].map({
            '√âlectronique': 'High-Tech',
            'Alimentaire': 'Consommation',
            'Papeterie': 'Bureau',
            'Mobilier': 'Maison',
            'V√™tements': 'Mode',
            'Jouets': 'Loisirs',
            'Sport': 'Loisirs',
            'Beaut√©': 'Bien-√™tre',
            'Sant√©': 'Bien-√™tre',
            'Automobile': 'Transport'
        })
        logger.info(f"  ‚úÖ {len(dim_categorie)} cat√©gories transform√©es")
        
        # FACT_VENTES
        logger.info("üíº Cr√©ation de FACT_VENTES...")
        ventes = self.dataframes['ventes'].copy()
        ventes['Date_vente'] = pd.to_datetime(ventes['Date_vente'])
        
        date_to_id = dict(zip(dim_temps['date_complete'], dim_temps['id_temps']))
        ventes['id_temps'] = ventes['Date_vente'].map(date_to_id)
        
        prod_to_id = dict(zip(dim_produit['Reference_Produit'], dim_produit['id_produit']))
        ventes['id_produit'] = ventes['Reference_Produit'].map(prod_to_id)
        
        ens_to_id = dict(zip(dim_enseigne['Enseigne'], dim_enseigne['id_enseigne']))
        ventes['id_enseigne'] = ventes['Enseigne'].map(ens_to_id)
        
        fact_ventes = pd.DataFrame({
            'id_vente': ventes['IDVente'],
            'id_temps': ventes['id_temps'],
            'id_produit': ventes['id_produit'],
            'id_enseigne': ventes['id_enseigne'],
            'quantite': ventes['Quantite'],
            'prix_unitaire': ventes['Prix_Total'] / ventes['Quantite'],
            'prix_total': ventes['Prix_Total'],
            'cout_estime': ventes['Prix_Total'] * 0.7,
            'marge_brute': ventes['Prix_Total'] * 0.3
        })
        logger.info(f"  ‚úÖ {len(fact_ventes)} transactions transform√©es")
        
        return dim_temps, dim_produit, dim_enseigne, dim_categorie, fact_ventes
    
    def load(self, dim_temps, dim_produit, dim_enseigne, dim_categorie, fact_ventes):
        logger.info("\nPHASE 3: CHARGEMENT DANS LE DATA WAREHOUSE")
        logger.info("="*60)
        
        # SQLite
        db_path = f"{self.target_path}datawarehouse.db"
        conn = sqlite3.connect(db_path)
        
        dim_temps.to_sql('dim_temps', conn, if_exists='replace', index=False)
        logger.info(f"‚úÖ DIM_TEMPS charg√©e")
        
        dim_produit.to_sql('dim_produit', conn, if_exists='replace', index=False)
        logger.info(f"‚úÖ DIM_PRODUIT charg√©e")
        
        dim_enseigne.to_sql('dim_enseigne', conn, if_exists='replace', index=False)
        logger.info(f"‚úÖ DIM_ENSEIGNE charg√©e")
        
        dim_categorie.to_sql('dim_categorie', conn, if_exists='replace', index=False)
        logger.info(f"‚úÖ DIM_CATEGORIE charg√©e")
        
        fact_ventes.to_sql('fact_ventes', conn, if_exists='replace', index=False)
        logger.info(f"‚úÖ FACT_VENTES charg√©e")
        
        conn.close()
        
        # Export CSV pour Power BI
        csv_path = '01_data/processed/'
        os.makedirs(csv_path, exist_ok=True)
        
        dim_temps.to_csv(f'{csv_path}dim_temps.csv', index=False)
        dim_produit.to_csv(f'{csv_path}dim_produit.csv', index=False)
        dim_enseigne.to_csv(f'{csv_path}dim_enseigne.csv', index=False)
        dim_categorie.to_csv(f'{csv_path}dim_categorie.csv', index=False)
        fact_ventes.to_csv(f'{csv_path}fact_ventes.csv', index=False)
        
        logger.info(f"‚úÖ Tables export√©es dans {csv_path}")
        logger.info(f"\n‚úÖ Data Warehouse cr√©√©: {db_path}")
    
    def run_etl(self):
        logger.info("üöÄ D√âMARRAGE DU PIPELINE ETL")
        self.extract()
        dim_temps, dim_produit, dim_enseigne, dim_categorie, fact_ventes = self.transform()
        self.load(dim_temps, dim_produit, dim_enseigne, dim_categorie, fact_ventes)
        logger.info("\nüéâ PIPELINE ETL TERMIN√â AVEC SUCC√àS!")

if __name__ == "__main__":
    etl = DataWarehouseETL()
    etl.run_etl()
